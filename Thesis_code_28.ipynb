{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code used for thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Data Science &\n",
    "Society at the School of Humanities and Digital Science of Tilburg University.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to be imported\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms\n",
    "import statistics\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file & preproces data\n",
    "# Source code: https://www.programiz.com/python-programming/reading-csv-files\n",
    "\n",
    "# Function to preproces csv-file into list with lists\n",
    "def preproces_data(file_name):\n",
    "    file_path_begin = '/Users/annasjouknijhof/Downloads/'\n",
    "    file_path_end = '.csv'\n",
    "    file_path_full = file_path_begin + file_name + file_path_end\n",
    "\n",
    "#create list with strings with all information for 1 company and 1 variable\n",
    "    data = []\n",
    "    with open(file_path_full, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row == []:\n",
    "                continue\n",
    "            else:\n",
    "                row = row[0].split(';')\n",
    "                #row = row[:5]\n",
    "                for i in range(2,5):\n",
    "                    try:\n",
    "                        row[i] = float(row[i])\n",
    "                    except:\n",
    "                        pass\n",
    "            data.append(row)\n",
    "    return(data)\n",
    "\n",
    "data = preproces_data('Import_to_Python_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to pandas dataframe and create column names\n",
    "df = pd.DataFrame(data).iloc[1:]\n",
    "df.columns = ['Company name', 'Variable', '2019', '2020', '2021', 'Mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentages of missing values\n",
    "# first check for each variable number of missing values per year\n",
    "\n",
    "# create list with variables\n",
    "def extract_list_with_variable(data):\n",
    "    variable_list = []\n",
    "    for i in range(1,21):\n",
    "        variable_list.append(data[i][1])\n",
    "    return(variable_list)\n",
    "\n",
    "variable_list = extract_list_with_variable(data)\n",
    "year = ['2019', '2020', '2021', 'Mean']\n",
    "\n",
    "# calculate aboslute number of missing values per year and variable\n",
    "def missing_values_per_year_and_company(year, variable):\n",
    "    list_results = []\n",
    "    for i in variable:\n",
    "        tmp_list = [i]\n",
    "        for j in year:\n",
    "            begin_index = variable.index(i)\n",
    "            indices = list(range(begin_index, 221400, 20)) \n",
    "            boolean_with_missing_value = df.iloc[indices][df.iloc[indices][j] == 'Missing value'] # selecteer rows with 'missing value'\n",
    "            missing_value_count = len(boolean_with_missing_value)\n",
    "            tmp_list.append(missing_value_count)\n",
    "        list_results.append(tmp_list)\n",
    "    return(pd.DataFrame(list_results, columns= ['Variable', '2019', '2020', '2021', 'Mean']))\n",
    "\n",
    "absolut_number = missing_values_per_year_and_company(year, variable_list)    \n",
    "\n",
    "# calculate percentages of missing values per year and variable\n",
    "def missing_values_per_year_and_company_percentage(year, variable):\n",
    "    list_results = []\n",
    "    for i in variable:\n",
    "        tmp_list = [i]\n",
    "        for j in year:\n",
    "            begin_index = variable.index(i)\n",
    "            indices = list(range(begin_index, 221400, 20))\n",
    "            boolean_with_missing_value = df.iloc[indices][df.iloc[indices][j] == 'Missing value'] # select from dataframe all booleans with missing value\n",
    "            missing_value_count = len(boolean_with_missing_value)\n",
    "            missing_value_percentage = missing_value_count / 11070 * 100\n",
    "            tmp_list.append(missing_value_percentage)\n",
    "        list_results.append(tmp_list)\n",
    "    return(pd.DataFrame(list_results, columns= ['Variable', '2019', '2020', '2021', 'Mean']))\n",
    "\n",
    "percentages = missing_values_per_year_and_company_percentage(year, variable_list)\n",
    "\n",
    "summ = percentages.mean(axis=1)\n",
    "percentages = percentages.round(1)\n",
    "percentages.to_excel('missing_value_percentages.xlsx')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table to see developments over time for missing values\n",
    "# source code: https://stackoverflow.com/questions/60625159/using-pandas-dataframe-melt-with-seaborn-barplot\n",
    "# source code: https://stackoverflow.com/questions/25068384/bbox-to-anchor-and-loc-in-matplotlib\n",
    "# source code: https://stackoverflow.com/questions/25239933/how-to-add-a-title-to-each-subplot\n",
    "# source code: https://datatofish.com/line-chart-python-matplotlib/\n",
    "# source code: https://www.geeksforgeeks.org/matplotlib-pyplot-savefig-in-python/\n",
    "# source code: https://stackoverflow.com/questions/11244514/modify-tick-label-text\n",
    "\n",
    "\n",
    "# add the column Mean to the line graph. The code for the graph already worked for the years 2019, 2020 and 2021\n",
    "# for this reasone the column Mean was renamed as 2022 to include it in the table. Eventually the name 2022 was renamed backwards\n",
    "# to the right name 'Mean'. This is not the beautiful option, but a quick fix that works fine.\n",
    "percentages['2022'] = percentages['Mean']\n",
    "\n",
    "# Melt dataframe to combine the all values\n",
    "percentages_melted = pd.melt(percentages, id_vars = ['Variable', 'Mean'], var_name = 'Year', value_name = 'Missing data percentage')\n",
    "# value_vars = all columns that are not taken as id_vars.\n",
    "# make line graph for each variable\n",
    "lines = percentages_melted.groupby('Variable')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, j in lines:\n",
    "    j.plot(x = 'Year', y = 'Missing data percentage', ax = ax, label = i) # ax = ax > add to ax, label = var. name\n",
    "\n",
    "# rename axes and include legend\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Missing data (%)')\n",
    "ax.set_title('Missing data percentages per year')\n",
    "ax.legend(loc=6, bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# rename label '2022' back to 'mean'\n",
    "labels = ['', '2019', '', '2020', '', '2021', '', 'mean', '']\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "plt.savefig('linegraph_missing_data_percentages.png', bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all missing values from the 'variable' column and replace it with the right variable name (which is possible since \n",
    "# the order of the variable names for each company is the same.)\n",
    "# first company (Apple) is skipped, because all variable names are available for this company and this worked better with my code.\n",
    "for i in range(20, len(df)):\n",
    "    variable_index = variable_list[i % 20]\n",
    "    df.iloc[i,1] = variable_index\n",
    "\n",
    "\n",
    "# drop the features that have an missing value rate larger than 10% (Current Ratio, Quick Ratio, Net Income 12M FWD, Gross profit Margin\n",
    "# and Current Assets - Total)\n",
    "# code used from https://www.askpython.com/python-modules/pandas/pandas-isin and from https://stackoverflow.com/questions/14057007/remove-rows-not-isinx for the isin() function. \n",
    "\n",
    "list_for_deletion = ['CURRENT RATIO', 'QUICK RATIO', 'NET INCOME 12M FWD', 'GROSS PROFIT MARGIN', 'CURRENT ASSETS - TOTAL']\n",
    "df = df[-df['Variable'].isin(list_for_deletion)].reset_index(drop=True)\n",
    "\n",
    "# select only the rows needed for further analysis and multiple imputation (index 0,1 and 5)\n",
    "\n",
    "df = df.iloc[:, [1,5]]\n",
    "\n",
    "# correct format of wrong number. This went wrong during the EDA in Excel.\n",
    "df.iloc[2,1] = 216.23333333\n",
    "\n",
    "# check for correct number of unique variables in column 'Variables'. This number must equal 15.\n",
    "#print(len(df['Variable'].unique()) == 15) #This is True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update list with variables for variables that are deleted\n",
    "def update_variable_list(variable_list):\n",
    "    variable_list_2 = []\n",
    "    for i in variable_list:\n",
    "        if i not in list_for_deletion:\n",
    "            variable_list_2.append(i)\n",
    "    return(variable_list_2)\n",
    "\n",
    "variable_list_2 = update_variable_list(variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data by pivotting the DataFrame, rename columns and reorder the variables, replace cells with 'Missing value' for NaN\n",
    "# code used from: https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns \n",
    "\n",
    "# add company_number column to dataframe\n",
    "repetitions = (len(df) // 15) + 2\n",
    "index_list = [i for i in range((len(df) // 15) + 2) for _ in range(15)]\n",
    "# or\n",
    "index_list = []\n",
    "for i in range(repetitions):\n",
    "    for _ in range(15):\n",
    "        index_list.append(i)\n",
    "\n",
    "index_list = index_list[:len(df)]\n",
    "df['Index'] = index_list\n",
    "\n",
    "# pivot table\n",
    "df = df.pivot(index = 'Index', columns='Variable')\n",
    "\n",
    "\n",
    "# rename and reorder columns\n",
    "columns_names = ['Board Size', 'COMMON EQTY % TOTAL ASSETS', 'DIVIDEND YIELD', 'EARNINGS PER SHR','EBIT & DEPRECIATION', 'EMPLOYEES', 'ESG Combined Score', 'NET SALES OR REVENUES', 'OPERATING INCOME','RETURN ON ASSETS', 'RETURN ON EQUITY - TOTAL (%)', 'RETURN ON INVESTED CAPITAL', 'TOTAL ASSETS', 'TOTAL DEBT', 'TOTAL DEBT % TOTAL CAPITAL/STD']\n",
    "df.columns = columns_names\n",
    "df = df[['ESG Combined Score', 'Board Size', 'COMMON EQTY % TOTAL ASSETS', 'DIVIDEND YIELD', 'EARNINGS PER SHR','EBIT & DEPRECIATION', 'EMPLOYEES', 'NET SALES OR REVENUES', 'OPERATING INCOME','RETURN ON ASSETS', 'RETURN ON EQUITY - TOTAL (%)', 'RETURN ON INVESTED CAPITAL', 'TOTAL ASSETS', 'TOTAL DEBT', 'TOTAL DEBT % TOTAL CAPITAL/STD']]\n",
    "\n",
    "# replace strings with 'Missing value' for Nan\n",
    "df = df.replace('Missing value', np.nan)\n",
    "\n",
    "# first: convert alle value in DataFrame to floats (instead of objects). (source code: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)\n",
    "df = df.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "# start describing the variables seperately and write it to a table\n",
    "describe_df = df.describe().round(1)\n",
    "describe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boxplot for each column (source code: https://stackoverflow.com/questions/51777217/how-to-plot-a-boxplot-for-each-column-in-a-dataframe)\n",
    "# since the scaling of the boxplots are not really the same, a seperate boxplot for each variable has been created.\n",
    "# created the Total debt % total capital/STD boxplot manually\n",
    "\n",
    "for column in df:\n",
    "    plt.figure()\n",
    "    df.boxplot([column], fontsize=10)\n",
    "    plot.title('Boxplot from ' + column)\n",
    "    #name_to_save_boxplot = 'Boxplot ' + column + '.png'\n",
    "    #plt.savefig(name_to_save_boxplot, bbox_inches = 'tight')   \n",
    "\n",
    "# summarize all boxplots together for thesis report in one image and save them as .PNG-file\n",
    "# source code: https://stackoverflow.com/questions/62404884/boxplot-with-different-y-axes-and-different-y-scales-in-seaborn\n",
    "# source code: https://www.educative.io/answers/what-is-a-subplots-in-matplotlib\n",
    "# source code: https://stackoverflow.com/questions/9012487/matplotlib-pyplot-savefig-outputs-blank-image\n",
    "# create subplots with 5 boxplots in each row\n",
    "'''\n",
    "column_names = df.columns.tolist()\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(20,25))\n",
    "for i, col in enumerate(column_names):\n",
    "    ax = sns.boxplot(y=df[col], ax=axes.flatten()[i]) # y = import data, ax = add to subplot, see source code 1 above. [i] number of subplot.\n",
    "    ax.set_ylim(df[col].min(), df[col].max()) # set limits to Y-axis\n",
    "    ax.set_ylabel(col)\n",
    "#name_to_save_boxplot = 'boxplot_summary.png'\n",
    "#plt.savefig(name_to_save_boxplot, bbox_inches = 'tight') \n",
    "plt.show() \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier analysis - extreme values\n",
    "\n",
    "# first, delete all (nine) rows without any values\n",
    "# provide a list of rows in the dataframe with only missing values (source code: https://datatofish.com/rows-with-nan-pandas-dataframe/)\n",
    "# rows_without_values = df[df.isnull().all(axis=1)] > this line is not necessary anymore\n",
    "index_rows_without_values = df[df.isnull().all(axis=1)].index\n",
    "deletion_list = []\n",
    "for i in index_rows_without_values:\n",
    "    deletion_list.append(i)\n",
    "# print(len(deletion_list)) > there are 9 companies without any values. Since these are only 9 companies, these companies are deleted from the data.\n",
    "df = df.drop(deletion_list)\n",
    "\n",
    "# delete illegal values and extreme values using the boxplots above and the describe_df DataFrame\n",
    "# e.g. negative values that cannot be negative of extreme and impossible values\n",
    "# values for percentages and ratio's are sooner considered outlier than variables with absolute values\n",
    "# source code used from: https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-where-column-matches-certain-value\n",
    "\n",
    "outliers_list = []\n",
    "# check for negative values\n",
    "'''for i in df.columns.tolist():\n",
    "    print(i)\n",
    "    df_t = df[df[i]<0]\n",
    "    df_t = df_t.sort_values(by=i, ascending=False)\n",
    "    print(len(df_t))\n",
    "    print(df_t)\n",
    "    print()'''\n",
    "\n",
    "#Net Sales/Revenues there are net sales/revenues value below zero, which is not possible\n",
    "\n",
    "# 'ESG Combined Score'\n",
    "# no illegal values\n",
    "\n",
    "# 'Board Size'\n",
    "# there are some extreme values that are possible outliers, sinces boards with more than 40 members are unrealistic\n",
    "df_t = df[df['Board Size']>40]\n",
    "df_t = df_t.sort_values(by='Board Size')\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "\n",
    "# 'COMMON EQTY % TOTAL ASSETS'\n",
    "# the first four are extreme outliers and hence unlikely\n",
    "#df_t = df[df['COMMON EQTY % TOTAL ASSETS']<-7500]\n",
    "#df_t = df_t.sort_values(by='COMMON EQTY % TOTAL ASSETS')\n",
    "#df_t = df_t.index.values.tolist()\n",
    "#outliers_list.append(df_t)\n",
    "\n",
    "#df_t = df[df['COMMON EQTY % TOTAL ASSETS']<0]\n",
    "#df_t = df_t.sort_values(by='COMMON EQTY % TOTAL ASSETS')\n",
    "#df_t = df_t.iloc[0:4,:]\n",
    "#df_t = df_t.index.values.tolist()\n",
    "#outliers_list.append(df_t)\n",
    "\n",
    "# 'DIVIDEND YIELD'\n",
    "# There is one extreme and unlikely outlier that might be deleted based on the boxplot\n",
    "\n",
    "df_t = df[df['DIVIDEND YIELD']>200]\n",
    "df_t = df_t.sort_values(by='DIVIDEND YIELD')\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "\n",
    "\n",
    "# 'EARNINGS PER SHR'\n",
    "# delete most unlikely values for values above 30k\n",
    "#df_t = df[df['EARNINGS PER SHR']>30000]\n",
    "#df_t = df_t.sort_values(by='EARNINGS PER SHR')\n",
    "#df_t = df_t.index.values.tolist()\n",
    "#outliers_list.append(df_t)\n",
    "\n",
    "# 'EBIT & DEPRECIATION'\n",
    "# There are extreme values but no illegal values or unlikely values\n",
    "\n",
    "# 'EMPLOYEES'\n",
    "# There are extreme values but no illegal values or unlikely vlaues\n",
    "\n",
    "# 'NET SALES OR REVENUES'\n",
    "# Net sales cannot be below zero, consequantially all values below zero are considered illegal outlier\n",
    "\n",
    "\n",
    "df_t = df[df['NET SALES OR REVENUES']<0]\n",
    "df_t = df_t.sort_values(by='NET SALES OR REVENUES')\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "\n",
    "\n",
    "# 'OPERATING INCOME'\n",
    "# There are extreme values but no illegal values or unlikely values\n",
    "\n",
    "# 'RETURN ON ASSETS'\n",
    "# two values with really low and hence unlikely return on assets with RoA lower than 650\n",
    "#df_t = df[df['RETURN ON ASSETS']<-650]\n",
    "#df_t = df_t.index.values.tolist()\n",
    "#outliers_list.append(df_t)\n",
    "\n",
    "'''\n",
    "# 'RETURN ON EQUITY - TOTAL (%)'\n",
    "# five values with really low and hence unlikely return on equity higher than 5000 or below 10000\n",
    "df_t = df[df['RETURN ON EQUITY - TOTAL (%)']<-10000]\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "df_t = df[df['RETURN ON EQUITY - TOTAL (%)']>5000]\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "\n",
    "# 'RETURN ON INVESTED CAPITAL' \n",
    "# four values with really low and hence unlikely return on invested capital below 2000\n",
    "df_t = df[df['RETURN ON INVESTED CAPITAL']<-2000]\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "\n",
    "# 'TOTAL ASSETS'\n",
    "# There are extreme values but no illegal values or unlikely values\n",
    "\n",
    "# 'TOTAL DEBT'\n",
    "# There are extreme values but no illegal values or unlikely values\n",
    "\n",
    "# 'TOTAL DEBT % TOTAL CAPITAL/STD'\n",
    "df_t = df[df['TOTAL DEBT % TOTAL CAPITAL/STD']<-4000]\n",
    "df_t = df_t.index.values.tolist()\n",
    "outliers_list.append(df_t)\n",
    "'''\n",
    "# delete all selected outliers\n",
    "\n",
    "outliers_list_dev = []\n",
    "for i in outliers_list:\n",
    "    for j in i:\n",
    "        if j not in outliers_list_dev:\n",
    "            outliers_list_dev.append(j)\n",
    "\n",
    "df = df.drop(outliers_list_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform outlier analysis and scaling analysis based on splitting the data where models are trained on training data instead of\n",
    "# full data\n",
    "X = df.drop(columns = ['ESG Combined Score'])\n",
    "y = df['ESG Combined Score']\n",
    "\n",
    "# split in training and other\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "train_data_complete = pd.concat([X_train, y_train], axis=1)\n",
    "test_data_complete = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile Transformation > tranform data distribution of all variables except the ESG Combined Score to normal distributed value\n",
    "# source code: https://towardsdatascience.com/5-data-transformers-to-know-from-scikit-learn-612bc48b8c89\n",
    "\n",
    "# perform on training data\n",
    "for i in train_data_complete.columns[1:15]:\n",
    "    quantile_transformer = QuantileTransformer(random_state=0,  output_distribution='normal')\n",
    "    j = i + '_trans'\n",
    "    train_data_complete[j] = pd.Series(quantile_transformer.fit_transform(np.array(train_data_complete[i]).reshape(-1, 1))[:,0])\n",
    "train_data_complete = train_data_complete.drop(train_data_complete.columns[1:16], axis=1)\n",
    "\n",
    "# perform on test data\n",
    "for i in test_data_complete.columns[1:15]:\n",
    "    quantile_transformer = QuantileTransformer(random_state=0,  output_distribution='normal')\n",
    "    j = i + '_trans'\n",
    "    test_data_complete[j] = pd.Series(quantile_transformer.fit_transform(np.array(test_data_complete[i]).reshape(-1, 1))[:,0]) # see source code\n",
    "test_data_complete = test_data_complete.drop(test_data_complete.columns[1:16], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize all variables\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale= StandardScaler()\n",
    "df_t4 = test_data_complete\n",
    "\n",
    "# standardization of dependent variables\n",
    "scaled_data = scale.fit_transform(df_t4) \n",
    "test_data_complete = pd.DataFrame(scaled_data, columns=test_data_complete.columns.tolist())\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale= StandardScaler()\n",
    "df_t4 = train_data_complete\n",
    "\n",
    "# standardization of dependent variables\n",
    "scaled_data = scale.fit_transform(df_t4) \n",
    "train_data_complete = pd.DataFrame(scaled_data, columns=train_data_complete.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize all variables except dependent variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for df in [test_data_complete, train_data_complete]:\n",
    "    scale= StandardScaler()\n",
    "    df_t4 = df.loc[:, df.columns != 'ESG Combined Score']\n",
    "    # standardization of dependent variables\n",
    "    scaled_data = scale.fit_transform(df_t4) \n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=df_t4.columns.tolist())\n",
    "\n",
    "    df = pd.concat([df.loc[:, df.columns == 'ESG Combined Score'], scaled_data], axis= 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semmatric logarithmicly scale all variable but the target variable (ESG Combined Score)\n",
    "# for negative values it take the logarithm of the absolute value + a small value of 0.00001 multiplied by the sign of the number\n",
    "# value of 0.00001 is added to make sure that none of the values equals zero.\n",
    "# source code: https://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-in-pandas\n",
    "# source code: https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "for df in [test_data_complete, train_data_complete]:\n",
    "    df_t = df.loc[:, df.columns != 'ESG Combined Score']\n",
    "    df_t = np.sign(df_t) * np.log10(np.abs(df_t) + 0.00001)\n",
    "    df = pd.concat([df.loc[:, df.columns == 'ESG Combined Score'], df_t], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semmatric logarithmicly scale all variable \n",
    "# for negative values it take the logarithm of the absolute value + a small value of 0.00001 multiplied by the sign of the number\n",
    "# value of 0.00001 is added to make sure that none of the values equals zero.\n",
    "# source code: https://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-in-pandas\n",
    "# source code: https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "for df in [test_data_complete, train_data_complete]:\n",
    "    df = np.sign(df) * np.log10(np.abs(df) + 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for overfitting\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete)\n",
    "train_data_complete = pd.DataFrame(imputed, columns=train_data_complete.columns)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(test_data_complete)\n",
    "test_data_complete = pd.DataFrame(imputed, columns=test_data_complete.columns)\n",
    "\n",
    "X_train = train_data_complete.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete['ESG Combined Score']\n",
    "\n",
    "X_test = test_data_complete.drop('ESG Combined Score', axis=1)\n",
    "y_test = test_data_complete['ESG Combined Score']\n",
    "\n",
    "# train model again\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE :', RMSE)\n",
    "y_pred_train = model.predict(X_train)\n",
    "MAE = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "MSE = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('R-squared: ' + str(r2_score(y_train, y_pred_train)))\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE :', RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, validation and test set\n",
    "# this is done by using the train_test function from the sklearn package twice\n",
    "# source code: https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c\n",
    "# source code: https://stackoverflow.com/questions/56166130/setting-seed-on-train-test-split-sklearn-python\n",
    "# we split the data in 80:20\n",
    "\n",
    "# select dependent and independent variables\n",
    "X = df.drop(columns = ['ESG Combined Score'])\n",
    "y = df['ESG Combined Score']\n",
    "\n",
    "# split in training and other\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "# split x_rest en y_rest in relationship of 50:50\n",
    "#X_valid, X_test, y_valid, y_test = train_test_split(X_rest,y_rest, test_size=0.5, random_state=454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge X en y trainingsdata sets\n",
    "train_data_complete = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection \n",
    "#source code: https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n",
    "#train_data_complete = train_data_complete.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "#train_data_complete = train_data_complete.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "#train_data_complete = train_data_complete.drop(['TOTAL ASSETS'], axis = 1)\n",
    "#train_data_complete = train_data_complete.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "#train_data_complete = train_data_complete.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1)\n",
    "#train_data_complete = train_data_complete.drop(['TOTAL DEBT'], axis = 1)\n",
    "# heatmap for all combinations\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(train_data_complete.corr(),annot=True)\n",
    "heatmap.set_title(\"Feature selection based on Pearson's correlation coefficients\")\n",
    "#plt.savefig('heatmap_1.png', bbox_inches = 'tight')\n",
    "'''\n",
    "# heatmap after dropping return on assets\n",
    "df = df.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "df = df.drop(['TOTAL ASSETS'], axis = 1)\n",
    "df = df.drop(['EBIT & DEPRECIATION'], axis = 1) # > dropped, because high correlation with two variables\n",
    "df = df.drop(['TOTAL DEBT'], axis = 1) \n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(df.corr(),annot=True)\n",
    "heatmap.set_title(\"Feature selection based on Pearson's correlation coefficients\")\n",
    "#plt.savefig('heatmap_1.png', bbox_inches = 'tight')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_complete.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete['ESG Combined Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HEREAFTING SEES ON EXECUTING MODELS\n",
    "\n",
    "R2_LR = []\n",
    "MAE_LR = []\n",
    "RMSE_LR = []\n",
    "\n",
    "R2_SVR = []\n",
    "MAE_SVR = []\n",
    "RMSE_SVR = []\n",
    "\n",
    "R2_RF = []\n",
    "MAE_RF = []\n",
    "RMSE_RF = []\n",
    "\n",
    "overfitting_R2_LR = []\n",
    "overfitting_MAE_LR = []\n",
    "overfitting_RMSE_LR = []\n",
    "\n",
    "overfitting_R2_SVR = []\n",
    "overfitting_MAE_SVR = []\n",
    "overfitting_RMSE_SVR = []\n",
    "\n",
    "overfitting_R2_RF = []\n",
    "overfitting_MAE_RF = []\n",
    "overfitting_RMSE_RF = []\n",
    "\n",
    "models = [777, 257, 8, 511, 2684]\n",
    "\n",
    "for i in models:\n",
    "\n",
    "    # select dependent and independent variables\n",
    "    X = df.drop(columns = ['ESG Combined Score'])\n",
    "    y = df['ESG Combined Score']\n",
    "\n",
    "    # split in training and other\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=i)\n",
    "\n",
    "    # random_state argument setted for reproducibility\n",
    "    # Code used from: https://towardsdatascience.com/iterative-imputation-with-scikit-learn-8f3eb22b1a38 \n",
    "\n",
    "    # drop variables\n",
    "    X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "    X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "    X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "    X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "    # FROM HERE: TRAIN MODELS\n",
    "\n",
    "    # Linear_regression\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    MAE_LR.append(MAE)\n",
    "    RMSE_LR.append(RMSE)\n",
    "    R2_LR.append(R2)\n",
    "\n",
    "        # check for overfitting\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    MAE = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "    MSE = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "    R2 = r2_score(y_train, y_pred_train)\n",
    "    \n",
    "    overfitting_MAE_LR.append(MAE)\n",
    "    overfitting_RMSE_LR.append(RMSE)\n",
    "    overfitting_R2_LR.append(R2)\n",
    "\n",
    "    # SVR\n",
    "    model = SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    MAE_SVR.append(MAE)\n",
    "    RMSE_SVR.append(RMSE)\n",
    "    R2_SVR.append(R2)\n",
    "\n",
    "        # check for overfitting\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    MAE = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "    MSE = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "    R2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "    overfitting_MAE_SVR.append(MAE)\n",
    "    overfitting_RMSE_SVR.append(RMSE)\n",
    "    overfitting_R2_SVR.append(R2)\n",
    "\n",
    "    # Random Forest\n",
    "    model = RandomForestRegressor(n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=2, max_depth=15)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    MAE_RF.append(MAE)\n",
    "    RMSE_RF.append(RMSE)\n",
    "    R2_RF.append(R2)\n",
    "\n",
    "        # check for overfitting\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    MAE = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "    MSE = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "    R2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "    overfitting_MAE_RF.append(MAE)\n",
    "    overfitting_RMSE_RF.append(RMSE)\n",
    "    overfitting_R2_RF.append(R2)\n",
    "\n",
    "print(R2_LR)\n",
    "print(MAE_LR)\n",
    "print(RMSE_LR)\n",
    "\n",
    "print(R2_SVR)\n",
    "print(MAE_SVR )\n",
    "print(RMSE_SVR )\n",
    "\n",
    "print(R2_RF )\n",
    "print(MAE_RF )\n",
    "print(RMSE_RF )\n",
    "\n",
    "print(overfitting_R2_LR )\n",
    "print(overfitting_MAE_LR )\n",
    "print(overfitting_RMSE_LR )\n",
    "\n",
    "print(overfitting_R2_SVR )\n",
    "print(overfitting_MAE_SVR )\n",
    "print(overfitting_RMSE_SVR)\n",
    "\n",
    "print(overfitting_R2_RF)\n",
    "print(overfitting_MAE_RF)\n",
    "print(overfitting_RMSE_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"MLR\")\n",
    "print()\n",
    "print(\"Std R2_LR:\", statistics.stdev(R2_LR))\n",
    "print(\"Mean R2_LR:\", statistics.mean(R2_LR))\n",
    "print()\n",
    "\n",
    "print(\"Std MAE_LR:\", statistics.stdev(MAE_LR))\n",
    "print(\"Mean MAE_LR:\", statistics.mean(MAE_LR))\n",
    "print()\n",
    "\n",
    "print(\"Std RMSE_LR:\", statistics.stdev(RMSE_LR))\n",
    "print(\"Mean RMSE_LR:\", statistics.mean(RMSE_LR))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_R2_LR:\", statistics.stdev(overfitting_R2_LR))\n",
    "print(\"Mean overfitting_R2_LR:\", statistics.mean(overfitting_R2_LR))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_MAE_LR:\", statistics.stdev(overfitting_MAE_LR))\n",
    "print(\"Mean overfitting_MAE_LR:\", statistics.mean(overfitting_MAE_LR))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_RMSE_LR:\", statistics.stdev(overfitting_RMSE_LR))\n",
    "print(\"Mean overfitting_RMSE_LR:\", statistics.mean(overfitting_RMSE_LR))\n",
    "print()\n",
    "\n",
    "print(\"SVR\")\n",
    "print()\n",
    "print(\"Std R2_SVR:\", statistics.stdev(R2_SVR))\n",
    "print(\"Mean R2_SVR:\", statistics.mean(R2_SVR))\n",
    "print()\n",
    "\n",
    "print(\"Std MAE_SVR:\", statistics.stdev(MAE_SVR))\n",
    "print(\"Mean MAE_SVR:\", statistics.mean(MAE_SVR))\n",
    "print()\n",
    "\n",
    "print(\"Std RMSE_SVR:\", statistics.stdev(RMSE_SVR))\n",
    "print(\"Mean RMSE_SVR:\", statistics.mean(RMSE_SVR))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_R2_SVR:\", statistics.stdev(overfitting_R2_SVR))\n",
    "print(\"Mean overfitting_R2_SVR:\", statistics.mean(overfitting_R2_SVR))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_MAE_SVR:\", statistics.stdev(overfitting_MAE_SVR))\n",
    "print(\"Mean overfitting_MAE_SVR:\", statistics.mean(overfitting_MAE_SVR))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_RMSE_SVR:\", statistics.stdev(overfitting_RMSE_SVR))\n",
    "print(\"Mean overfitting_RMSE_SVR:\", statistics.mean(overfitting_RMSE_SVR))\n",
    "print()\n",
    "\n",
    "print(\"RF\")\n",
    "print()\n",
    "print(\"Std R2_RF:\", statistics.stdev(R2_RF))\n",
    "print(\"Mean R2_RF:\", statistics.mean(R2_RF))\n",
    "print()\n",
    "\n",
    "print(\"Std MAE_RF:\", statistics.stdev(MAE_RF))\n",
    "print(\"Mean MAE_RF:\", statistics.mean(MAE_RF))\n",
    "print()\n",
    "\n",
    "print(\"Std RMSE_RF:\", statistics.stdev(RMSE_RF))\n",
    "print(\"Mean RMSE_RF:\", statistics.mean(RMSE_RF))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_R2_RF:\", statistics.stdev(overfitting_R2_RF))\n",
    "print(\"Mean overfitting_R2_RF:\", statistics.mean(overfitting_R2_RF))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_MAE_RF:\", statistics.stdev(overfitting_MAE_RF))\n",
    "print(\"Mean overfitting_MAE_RF:\", statistics.mean(overfitting_MAE_RF))\n",
    "print()\n",
    "\n",
    "print(\"Std overfitting_RMSE_RF:\", statistics.stdev(overfitting_RMSE_RF))\n",
    "print(\"Mean overfitting_RMSE_RF:\", statistics.mean(overfitting_RMSE_RF))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_test = X_test.drop(['TOTAL DEBT'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state argument setted for reproducibility\n",
    "# Code used from: https://towardsdatascience.com/iterative-imputation-with-scikit-learn-8f3eb22b1a38 \n",
    "\n",
    "train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_train)\n",
    "train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_test)\n",
    "train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data back\n",
    "\n",
    "X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete_train['ESG Combined Score']\n",
    "\n",
    "X_test = train_data_complete_test.drop('ESG Combined Score', axis=1)\n",
    "y_test = train_data_complete_test['ESG Combined Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code: https://medium.com/machine-learning-with-python/multiple-linear-regression-implementation-in-python-2de9b303fc0c\n",
    "# source code for time: https://github.com/idr4n/naivebayes-email-author\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Intercept: \", model.intercept_)\n",
    "print(\"Coefficients:\",pd.DataFrame(list(zip(X, model.coef_)))) # see source code\n",
    "\n",
    "#Prediction of test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code: https://medium.com/machine-learning-with-python/multiple-linear-regression-implementation-in-python-2de9b303fc0c\n",
    "# source code: https://www.statology.org/r-squared-in-python/\n",
    "\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE :', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for overfitting\n",
    "\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "MAE = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "MSE = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('R-squared: ' + str(r2_score(y_train, y_pred_train)))\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE :', RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector regression - standard default settings model\n",
    "# source code: https://www.section.io/engineering-education/support-vector-regression-in-python/\n",
    "\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE :', RMSE)\n",
    "\n",
    "print('Check for overfitting: ')\n",
    "print()\n",
    "y_pred_train = model.predict(X_train)\n",
    "MAE = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "MSE = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('R-squared: ' + str(r2_score(y_train, y_pred_train)))\n",
    "print('MAE:', MAE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE :', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code: https://www.projectpro.io/recipes/find-optimal-parameters-using-gridsearchcv-for-regression\n",
    "# source code: https://www.kaggle.com/code/aniketyadav1/svm-and-hyper-parameter-tuning\n",
    "# source code: https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/\n",
    "# source code: https://medium.com/it-paragon/support-vector-machine-regression-cf65348b6345\n",
    "# plus documentation to learn the parameters\n",
    "\n",
    "# define hyperparameters to tune\n",
    "parameters = {'C': [0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "\n",
    "# create a GridSearchCV object\n",
    "grid_search = GridSearchCV(model, parameters, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters and the corresponding score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "# adding positive=True results in a lower R-squared\n",
    "\n",
    "# still not finish loading after 8555m and 37.5s\n",
    "# than: tried manually\n",
    "# than tried again without the kernel 'poly'\n",
    "# than check if poly is better with other optimal parameters as constants and calculate for default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual hyperparameter tuning\n",
    "C = [0.1, 1, 10, 100]\n",
    "gamma = [1,0.1,0.01,0.001]\n",
    "kernel = ['rbf', 'sigmoid', 'linear', 'poly']\n",
    "epsilon = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "dataframe_input_list = []\n",
    "for c in C:\n",
    "    for g in gamma:\n",
    "        for k in kernel:\n",
    "            tmp_result_list = []\n",
    "            tmp_result_list.append(c)\n",
    "            tmp_result_list.append(g)\n",
    "            tmp_result_list.append(k)\n",
    "            model = SVR(C=c, gamma=g, kernel=k )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_training = model.predict(X_train)\n",
    "            tmp_result_list.append(r2_score(y_train, y_pred_training))\n",
    "            tmp_result_list.append(r2_score(y_test, y_pred_test))\n",
    "            tmp_result_list.append(r2_score(y_test, y_pred_test) - r2_score(y_train, y_pred_training))\n",
    "            dataframe_input_list.append(tmp_result_list)\n",
    "            print(dataframe_input_list)\n",
    "            print(tmp_result_list)\n",
    "\n",
    "dataframe_output = pd.DataFrame(dataframe_input_list)\n",
    "dataframe_output\n",
    "\n",
    "# Conclusion: the Poly-kernel is problematic due to loading times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try GridSearchCV again, but without Poly-kernel and with more hyperparameters, as the model now works fast\n",
    "parameters = {'C': [0.0625, 0.5, 1, 2, 16, 128], 'gamma': [0.001953, 0.03125, 0.125, 0.5, 1], 'kernel': ['rbf', 'sigmoid', 'linear'], 'epsilon': [0.1, 0.25, 0.5, 1]}\n",
    "grid_search = GridSearchCV(model, parameters, scoring='r2', n_jobs=-1) \n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search  >  final\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_features': [1, 2, 3, 4, 5, 6],\n",
    "    'min_samples_leaf': [1,5, 25, 50, 100, 200, 300, 400, 500, 600],\n",
    "    'min_samples_split': [1, 2, 3, 4, 5, 6],\n",
    "    'n_estimators': [1,5,10,50,100, 200, 300, 400, 500, 750, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "model = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "random_search = RandomizedSearchCV(estimator = model, scoring='r2', param_distributions= param_grid, n_iter=10000, n_jobs = -1)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)\n",
    "\n",
    "\n",
    "# {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 15}\n",
    "# 0.3531047891460169\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result van een gridsearchCV met de onderstaande parameters\n",
    "''' \n",
    "op trainingsdata zonder validatiedata\n",
    "'max_depth': [5, 10, 25, 50],\n",
    "'max_features': [2,4,6],\n",
    "'min_samples_leaf': [1,5,10],\n",
    "'min_samples_split': [2,4,6],\n",
    "'n_estimators': [1,5,10,50,100]\n",
    "{'max_depth': 25, 'max_features': 2, 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100}'''\n",
    "# took 83m 39s\n",
    "\n",
    "# 1530 minutes training time > stopped with following parameters\n",
    "'''   \n",
    "'max_depth': [5, 10, 25, 50],\n",
    "'max_features': [2,4,6],\n",
    "'min_samples_leaf': [1,2,5,10,50,100,300],\n",
    "'min_samples_split': [2,4,6],\n",
    "'n_estimators': [1,5,10,50,100,300]'''\n",
    "\n",
    "\n",
    "'''\n",
    "Resulted in:\n",
    "'max_depth': [5, 15, 25, 35, 50],\n",
    "'max_features': [2,4,6],\n",
    "'min_samples_leaf': [1,5, 25, 100, 600],\n",
    "'min_samples_split': [2,4,6],\n",
    "'n_estimators': [1,5,10,50,100]\n",
    "    \n",
    "Results for above hyperparameters on training data (no validation data present)\n",
    "{'max_depth': 15,\n",
    " 'max_features': 6,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 2,\n",
    " 'n_estimators': 100}\n",
    " >>> minimum sample leaf tunen geeft een veel beter resultaat met minder overfitting'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplot with three plots including all residual plots for each model\n",
    "# source code: https://www.geeksforgeeks.org/plot-multiple-plots-in-matplotlib/ & documentation of subplots function\n",
    "# source code for residual plot: https://datagy.io/seaborn-residplot/\n",
    "# used for averaging scores: https://www.javatpoint.com/how-to-add-two-lists-in-python \n",
    "\n",
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=100, min_samples_leaf=1, max_features = 6, min_samples_split = 2, random_state=19)]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x = 0\n",
    "for model in models:\n",
    "    '''prediction_lists = []\n",
    "\n",
    "    for i in [8, 777, 257, 511, 2684]:\n",
    "        # select dependent and independent variables\n",
    "        X = df.drop(columns = ['ESG Combined Score'])\n",
    "        y = df['ESG Combined Score']\n",
    "\n",
    "        # split in training and other\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=i)\n",
    "\n",
    "        # random_state argument setted for reproducibility\n",
    "        # Code used from: https://towardsdatascience.com/iterative-imputation-with-scikit-learn-8f3eb22b1a38 \n",
    "\n",
    "        # drop variables\n",
    "        X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "        X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "        X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "        X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "        X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "        X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "        X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "        X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "        X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) # HAS 2 IPV 1 CORRELATING VARIABLES\n",
    "        X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "        X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "        X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "        train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "        train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        imputer = IterativeImputer(random_state=42)\n",
    "        imputed = imputer.fit_transform(train_data_complete_train)\n",
    "        train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "        imputer = IterativeImputer(random_state=42)\n",
    "        imputed = imputer.fit_transform(train_data_complete_test)\n",
    "        train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "        X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "        y_train = train_data_complete_train['ESG Combined Score']\n",
    "\n",
    "        X_test = train_data_complete_test.drop('ESG Combined Score', axis=1)\n",
    "        y_test = train_data_complete_test['ESG Combined Score']\n",
    "\n",
    "        fit = model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        prediction_lists.append(list(y_pred))\n",
    "    lists_total = []\n",
    "    for i in range(len(prediction_lists[0])):\n",
    "        total = sum(prediction_lists[j][i] for j in range(len(prediction_lists)))\n",
    "        lists_total.append(total / 5)\n",
    "    y_pred = [value / len(prediction_lists) for value in y_pred]\n",
    "    # after this y_pred is averaged for the model'''\n",
    "\n",
    "    residuals = y_test - y_pred\n",
    "    sns.regplot(data=residuals, x=y_pred, y=residuals, ax=ax[x])\n",
    "    ax[x].axhline(y=0, color='red')\n",
    "    ax[x].set_xlabel('Predicted value')\n",
    "    ax[x].set_ylabel('Residual')\n",
    "    if x == 0:\n",
    "        ax[x].set_title('Residual plot for MLR model')\n",
    "    elif x == 1:\n",
    "        ax[x].set_title('Residual plot for SVR model')\n",
    "    else:\n",
    "        ax[x].set_title('Residual plot for RF')\n",
    "    x += 1\n",
    "plt.savefig('residual_plots_all_models', bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplot with three plots including all residual plots for each model\n",
    "# source code: https://www.geeksforgeeks.org/plot-multiple-plots-in-matplotlib/ & documentation of subplots function\n",
    "# source code for residual plot: https://datagy.io/seaborn-residplot/\n",
    "# used for averaging scores: https://www.javatpoint.com/how-to-add-two-lists-in-python \n",
    "\n",
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=1000, min_samples_leaf=1, max_features = 2, min_samples_split = 2, random_state=19)]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x = 0\n",
    "for model in models:\n",
    "    fit = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    sns.regplot(data=residuals, x=y_pred, y=residuals, ax=ax[x])\n",
    "    ax[x].axhline(y=0, color='red')\n",
    "    ax[x].set_xlabel('Predicted value')\n",
    "    ax[x].set_ylabel('Residual')\n",
    "    if x == 0:\n",
    "        ax[x].set_title('Residual plot for MLR model')\n",
    "    elif x == 1:\n",
    "        ax[x].set_title('Residual plot for SVR model')\n",
    "    else:\n",
    "        ax[x].set_title('Residual plot for RF')\n",
    "    x += 1\n",
    "plt.savefig('residual_plots_all_models', bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute Breusch-Pagan test to test statistically for the presence of heteroscedasticity \n",
    "# source code: https://www.statology.org/breusch-pagan-test-python/\n",
    "# source code: https://www.geeksforgeeks.org/how-to-perform-a-breusch-pagan-test-in-python/\n",
    "\n",
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=1000, min_samples_leaf=1, max_features = 2, min_samples_split = 2, random_state=19)] \n",
    "for i, model in enumerate(models):\n",
    "    if i == 0:\n",
    "        print('MLR model')\n",
    "    elif i == 1:\n",
    "        print('SVR model')\n",
    "    else:\n",
    "        print('RF')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    names = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "    test_result = sms.het_breuschpagan(residuals, X_test) # X_test, because suspected for heteroscedasticity\n",
    "    print(lzip(names, test_result)) # creates tupple with name and test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis > 4 groups based on ESG rating\n",
    "# test for difference in companies with lower and higher ESG-ratings\n",
    "X = df.drop(columns = ['ESG Combined Score'])\n",
    "y = df['ESG Combined Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_train)\n",
    "train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_test)\n",
    "train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete_train['ESG Combined Score']\n",
    "\n",
    "X_test = train_data_complete_test.drop('ESG Combined Score', axis=1)\n",
    "y_test = train_data_complete_test['ESG Combined Score']\n",
    "# first create two lists with all the y and X rows.\n",
    "# check performance on four classes of ESG scores (lowest 25%, 25 till 50% 50 till 75% and 75% percent)\n",
    "# source code to select the right columns: https://stackoverflow.com/questions/34243194/filter-rows-of-pandas-dataframe-whose-values-are-lower-than-0\n",
    "# select break points \n",
    "Q1 = y_test.quantile(0.25)\n",
    "median = y_test.median()\n",
    "Q3 = y_test.quantile(0.75)\n",
    "\n",
    "# select groups in y_test set based on break points\n",
    "y_under_25 = y_test[y_test <= Q1]\n",
    "y_between_25_and_50 = y_test[(y_test > Q1) & (y_test <= median)]\n",
    "y_between_50_and_75 = y_test[(y_test > median) & (y_test <= Q3)]\n",
    "y_over_75 = y_test[y_test > Q3]\n",
    "X_under_25 = None\n",
    "X_between_25_and_50 = None\n",
    "X_between_50_and_75 = None\n",
    "X_over_75 = None\n",
    "X_groups = [X_under_25, X_between_25_and_50, X_between_50_and_75, X_over_75]\n",
    "# select right independent values per group \n",
    "y_groups = [y_under_25, y_between_25_and_50, y_between_50_and_75, y_over_75]\n",
    "\n",
    "for i, group in enumerate(y_groups):\n",
    "    group_indices = group.index.tolist()\n",
    "    X_groups[i] = X_test[X_test.index.isin(group_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models on training data set\n",
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=1000, min_samples_leaf=1, max_features = 2, min_samples_split = 2, random_state=19)] \n",
    "for model in models:\n",
    "    X = df.drop(columns = ['ESG Combined Score'])\n",
    "    y = df['ESG Combined Score']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "    X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "    X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "    train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "    train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_train)\n",
    "    train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_test)\n",
    "    train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "    X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "    y_train = train_data_complete_train['ESG Combined Score']\n",
    "    model.fit(X_train, y_train)\n",
    "    for i in range(0,4):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print('quantile ' + str(i))\n",
    "        print()\n",
    "        MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "        print('MAE:', MAE)\n",
    "        print('RMSE :', RMSE)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis > based on ESG with only two groups: higher and lower \n",
    "# test for difference in companies with lower and higher ESG-ratings\n",
    "X = df.drop(columns = ['ESG Combined Score'])\n",
    "y = df['ESG Combined Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_train)\n",
    "train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_test)\n",
    "train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete_train['ESG Combined Score']\n",
    "# first create two lists with all the y and X rows.\n",
    "# check performance on four classes of ESG scores (lowest 25%, 25 till 50% 50 till 75% and 75% percent)\n",
    "# source code to select the right columns: https://stackoverflow.com/questions/34243194/filter-rows-of-pandas-dataframe-whose-values-are-lower-than-0\n",
    "# select break point\n",
    "median = y_test.median()\n",
    "\n",
    "# select groups in y_test set based on break points\n",
    "y_lower = y_test[y_test < median]\n",
    "y_higher = y_test[y_test >= median]\n",
    "X_lower = None\n",
    "X_higher = None\n",
    "X_groups = [X_lower, X_higher]\n",
    "# select right independent values per group \n",
    "y_groups = [y_lower, y_higher]\n",
    "\n",
    "for i, group in enumerate(y_groups):\n",
    "    group_indices = group.index.tolist()\n",
    "    X_groups[i] = X_test[X_test.index.isin(group_indices)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=100, min_samples_leaf=1, max_features = 6, min_samples_split = 2, random_state=19)] \n",
    "for model in models:\n",
    "    X = df.drop(columns = ['ESG Combined Score'])\n",
    "    y = df['ESG Combined Score']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "    X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "    X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "    train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "    train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_train)\n",
    "    train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_test)\n",
    "    train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "    X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "    y_train = train_data_complete_train['ESG Combined Score']\n",
    "    model.fit(X_train, y_train)\n",
    "    for i in range(0,2):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print('split_number: ' + str(i))\n",
    "        print()\n",
    "        MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "        print('MAE:', MAE)\n",
    "        print('RMSE :', RMSE)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis > 4 groups based on revenue\n",
    "# test for difference between larger and smaller companies based on revenues\n",
    "X = df.drop(columns = ['ESG Combined Score'])\n",
    "y = df['ESG Combined Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_train)\n",
    "train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_test)\n",
    "train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete_train['ESG Combined Score']\n",
    "# source code to select the right columns: https://stackoverflow.com/questions/34243194/filter-rows-of-pandas-dataframe-whose-values-are-lower-than-0\n",
    "# select break points \n",
    "Q1 = X_test['NET SALES OR REVENUES'].quantile(0.25)\n",
    "median = X_test['NET SALES OR REVENUES'].median()\n",
    "Q3 = X_test['NET SALES OR REVENUES'].quantile(0.75)\n",
    "\n",
    "# select groups in y_test set based on break points\n",
    "y_under_25 = None\n",
    "y_between_25_and_50 = None\n",
    "y_between_50_and_75 = None\n",
    "y_over_75 = None\n",
    "\n",
    "X_under_25 = X_test[X_test['NET SALES OR REVENUES'] <= Q1]\n",
    "X_between_25_and_50 = X_test[(X_test['NET SALES OR REVENUES'] > Q1) & (X_test['NET SALES OR REVENUES'] <= median)]\n",
    "X_between_50_and_75 = X_test[(X_test['NET SALES OR REVENUES'] > median) & (X_test['NET SALES OR REVENUES'] <= Q3)]\n",
    "X_over_75 = X_test[X_test['NET SALES OR REVENUES'] > Q3]\n",
    "X_groups = [X_under_25, X_between_25_and_50, X_between_50_and_75, X_over_75]\n",
    "y_groups = [y_under_25, y_between_25_and_50, y_between_50_and_75, y_over_75]\n",
    "\n",
    "# select right independent values per group \n",
    "for i, group in enumerate(X_groups):\n",
    "    group_indices = group.index.tolist()\n",
    "    y_groups[i] = y_test[y_test.index.isin(group_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=100, min_samples_leaf=1, max_features = 6, min_samples_split = 2, random_state=19)] \n",
    "for model in models:\n",
    "    X = df.drop(columns = ['ESG Combined Score'])\n",
    "    y = df['ESG Combined Score']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "    X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "    y_train = train_data_complete_train['ESG Combined Score']\n",
    "\n",
    "    X_test = train_data_complete_test.drop('ESG Combined Score', axis=1)\n",
    "    y_test = train_data_complete_test['ESG Combined Score']\n",
    "\n",
    "    X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "    X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "    train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "    train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_train)\n",
    "    train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_test)\n",
    "    train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "    X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "    y_train = train_data_complete_train['ESG Combined Score']\n",
    "    model.fit(X_train, y_train)\n",
    "    for i in range(0,4):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print('quantile ' + str(i))\n",
    "        print()\n",
    "        MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "        print('MAE:', MAE)\n",
    "        print('RMSE :', RMSE)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis > based on ESG with only two groups: higher and lower \n",
    "# test for difference in companies with lower and higher ESG-ratings\n",
    "X = df.drop(columns = ['ESG Combined Score'])\n",
    "y = df['ESG Combined Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_train)\n",
    "train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "imputed = imputer.fit_transform(train_data_complete_test)\n",
    "train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "y_train = train_data_complete_train['ESG Combined Score']\n",
    "# first create two lists with all the y and X rows.\n",
    "# check performance on four classes of ESG scores (lowest 25%, 25 till 50% 50 till 75% and 75% percent)\n",
    "# source code to select the right columns: https://stackoverflow.com/questions/34243194/filter-rows-of-pandas-dataframe-whose-values-are-lower-than-0\n",
    "# select break point\n",
    "median = X_test['NET SALES OR REVENUES'].median()\n",
    "print(median)\n",
    "# select groups in y_test set based on break points\n",
    "y_lower = None\n",
    "y_higher = None\n",
    "X_lower = X_test[X_test['NET SALES OR REVENUES'] < median]\n",
    "X_higher = X_test[X_test['NET SALES OR REVENUES'] >= median]\n",
    "X_groups = [X_lower, X_higher]\n",
    "y_groups = [y_lower, y_higher]\n",
    "\n",
    "for i, group in enumerate(X_groups):\n",
    "    group_indices = group.index.tolist()\n",
    "    y_groups[i] = y_test[y_test.index.isin(group_indices)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression(), SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf'), RandomForestRegressor(max_depth = 15, n_estimators=100, min_samples_leaf=1, max_features = 6, min_samples_split = 2, random_state=19)] \n",
    "for model in models:\n",
    "    X = df.drop(columns = ['ESG Combined Score'])\n",
    "    y = df['ESG Combined Score']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=777)\n",
    "\n",
    "    X_train = X_train.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_train = X_train.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_train = X_train.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_train = X_train.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_train = X_train.drop(['TOTAL DEBT'], axis = 1)\n",
    "\n",
    "    X_test = X_test.drop(['RETURN ON ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON INVESTED CAPITAL'], axis = 1)\n",
    "    X_test = X_test.drop(['RETURN ON EQUITY - TOTAL (%)'], axis = 1) \n",
    "    X_test = X_test.drop(['TOTAL ASSETS'], axis = 1)\n",
    "    X_test = X_test.drop(['EBIT & DEPRECIATION'], axis = 1)\n",
    "    X_test = X_test.drop(['TOTAL DEBT'], axis = 1)\n",
    "    train_data_complete_train = pd.concat([X_train, y_train], axis=1)\n",
    "    train_data_complete_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_train)\n",
    "    train_data_complete_train = pd.DataFrame(imputed, columns=train_data_complete_train.columns)\n",
    "\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    imputed = imputer.fit_transform(train_data_complete_test)\n",
    "    train_data_complete_test = pd.DataFrame(imputed, columns=train_data_complete_test.columns)\n",
    "\n",
    "\n",
    "    X_train = train_data_complete_train.drop('ESG Combined Score', axis=1)\n",
    "    y_train = train_data_complete_train['ESG Combined Score']\n",
    "    model.fit(X_train, y_train)\n",
    "    for i in range(0,2):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print('split_number: ' + str(i))\n",
    "        print()\n",
    "        MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        print('R-squared: ' + str(r2_score(y_test, y_pred)))\n",
    "        print('MAE:', MAE)\n",
    "        print('RMSE :', RMSE)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance linear regression - using coefficients of the model\n",
    "# source code: https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "# source code to add labels: https://stackoverflow.com/questions/21910986/why-set-xticks-doesnt-set-the-labels-of-ticks\n",
    "# source code to change axes from vertical to horizontal: https://www.geeksforgeeks.org/matplotlib-pyplot-barh-function-in-python/\n",
    "\n",
    "from matplotlib import pyplot\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "importance = model.coef_\n",
    "importance = np.sort(importance)\n",
    "fig, ax = pyplot.subplots()\n",
    "fig.set_size_inches(2.5, 3.5)\n",
    "plt.title(label='MLR model feature importance')\n",
    "ax.barh(range(len(importance)), importance) # y-axis = range, values = importance\n",
    "ax.set_yticks(range(len(importance))) # tick size\n",
    "ax.set_yticklabels(X_train.columns) # tick value\n",
    "plt.savefig('MLR model feature importance', bbox_inches = 'tight') \n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance analysis - Support Vector Regression - using Permutation feature importance since no model specific \n",
    "# options are available\n",
    "# source code: https://stackoverflow.com/questions/41592661/determining-the-most-contributing-features-for-svm-classifier-in-sklearn\n",
    "# source code: https://stackoverflow.com/questions/70467781/feature-importance-with-svr\n",
    "\n",
    "model = SVR(C=16, epsilon=1, gamma=0.125, kernel='rbf')\n",
    "model = RandomForestRegressor(max_depth = 15, n_estimators=1000, min_samples_leaf=1, max_features = 2, min_samples_split = 2, random_state=19)\n",
    "model.fit(X_train, y_train)\n",
    "results = permutation_importance(model, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "importance = results.importances_mean\n",
    "indices = np.argsort(importance)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2.5, 3.5)\n",
    "plt.title(label='RF model feature importance')\n",
    "ax.barh(range(len(importance)), importance[indices]) # range(len(importance)) provides place on y-axis, imporatance[indices] the order\n",
    "ax.set_yticks(range(len(importance)))\n",
    "ax.set_yticklabels(np.array(X_train.columns)[indices]) \n",
    "plt.savefig('RF model feature importance', bbox_inches = 'tight')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance analysis - Random Forest - using the importance output of the model\n",
    "# sourcde code: copied without changes from line 6 and further on may 11th 2023 from https://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html\n",
    "# source code: https://stackoverflow.com/questions/44511636/plot-feature-importance-with-feature-names\n",
    "# source code to resize graph\n",
    "\n",
    "# not useful anymore\n",
    "\n",
    "model = RandomForestRegressor(max_depth = 15, n_estimators=100, min_samples_leaf=1, max_features = 6, min_samples_split = 2, random_state=19)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(range(len(importances)), importances[indices])\n",
    "ax.set_yticks(range(len(importances)))\n",
    "fig.set_size_inches(2.5, 3.5)\n",
    "plt.title(label='RF feature importance')\n",
    "ax.set_yticklabels(np.array(X_train.columns)[indices])\n",
    "plt.savefig('feature-importance - random forest', bbox_inches = 'tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for difference between training and test set\n",
    "df_train = X_train.describe()\n",
    "df_test = X_test.describe()\n",
    "print(df_train)\n",
    "print()\n",
    "print('df_test')\n",
    "print(df_test)\n",
    "print(\n",
    "\n",
    ")\n",
    "[print('minus_table')]\n",
    "print(df_train - df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ALL CODE AFTER THIS LINE IS NOT USED IN THE FINAL THESIS BUT ARE MAINLY TRY-OUTS (NOT USED IN THE THESIS)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual hyperparameter tuning\n",
    "# in principle take default, unless specified otherwise\n",
    "# source code: https://realpython.com/iterate-through-dictionary-python/#iterating-through-items\n",
    "# source code: https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments\n",
    "# optimize as discussed for R-squared\n",
    "\n",
    "parameters = {'C': [0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid', 'linear'], 'epsilon': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "for name, value in parameters.items():\n",
    "    for i in value:\n",
    "        model = SVR()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_training = model.predict(X_train)\n",
    "        print('Model based on: ' + name + ' = ' + str(i))\n",
    "        print('R-squared on test set: ' + str(r2_score(y_train, y_pred_training)))\n",
    "        print('R-squared on test set: ' + str(r2_score(y_test, y_pred_test)))\n",
    "        print('The difference between R-squared scores is: ' + str(r2_score(y_test, y_pred_test) - r2_score(y_train, y_pred_training)))\n",
    "        print()\n",
    "\n",
    "# initial model, but overfitting model = SVR(C= 100, gamma= 0.1, kernel= 'rbf', epsilon= 0.3 ) > C=1 geeft ook prima result en c=10 ook, maar die overfit ook wel\n",
    "# solution, decrease gamma to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'C': [0.1, 1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid', 'linear'], 'epsilon': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "C = [0.1, 1, 10, 100]\n",
    "gamma = [1,0.1,0.01,0.001]\n",
    "kernel = ['rbf', 'sigmoid', 'linear']\n",
    "epsilon = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "dataframe_input_list = []\n",
    "for c in C:\n",
    "    for g in gamma:\n",
    "        for k in kernel:\n",
    "            tmp_result_list = []\n",
    "            tmp_result_list.append(c)\n",
    "            tmp_result_list.append(g)\n",
    "            tmp_result_list.append(k)\n",
    "            model = SVR(C=c, gamma=g, kernel=k )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_training = model.predict(X_train)\n",
    "            tmp_result_list.append(r2_score(y_train, y_pred_training))\n",
    "            tmp_result_list.append(r2_score(y_test, y_pred_test))\n",
    "            tmp_result_list.append(r2_score(y_test, y_pred_test) - r2_score(y_train, y_pred_training))\n",
    "            dataframe_input_list.append(tmp_result_list)\n",
    "            print(dataframe_input_list)\n",
    "            print(tmp_result_list)\n",
    "\n",
    "dataframe_output = pd.DataFrame(dataframe_input_list)\n",
    "dataframe_output\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.1, 1, 10, 100]\n",
    "gamma = [1,0.1,0.01,0.001]\n",
    "kernel = ['rbf', 'sigmoid', 'linear']\n",
    "epsilon = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "dataframe_input_list = []\n",
    "x = 0\n",
    "for c in C:\n",
    "    for g in gamma:\n",
    "        for k in kernel:\n",
    "            x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# This method was tried, but based on literature this method is not the best option to use so it is not used in the rest of the \n",
    "# project\n",
    "\n",
    "# Apply multiple imputation for missing values with KNN-method\n",
    "# This option doesn't work, since values must be normalized, but due to a wide range of possible values this unfortunately\n",
    "# is impossible.\n",
    "\n",
    "\n",
    "# data used from: https://datagy.io/pandas-normalize-column/ >>> let op: dit moet ook nog per column gaan gebeuren!!!!\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df)\n",
    "scaled = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled, columns=df.columns)\n",
    "scaled_df\n",
    "\n",
    "# code used, based on Python documentation (https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer)\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "df_imputed_KNNImputer = imputer.fit_transform(df)\n",
    "df_imputed_KNNImputer = pd.DataFrame(df_imputed_KNNImputer)\n",
    "df_imputed_KNNImputer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe where probable outliers are deleted\n",
    "def test(var_name):\n",
    "    df_t_list = []\n",
    "    for i in var_name:   \n",
    "        #determine ranges\n",
    "        Q1 = df[i].quantile(0.25)\n",
    "        Q3 = df[i].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        # filter outliers from dataset\n",
    "        filter = (df[i] >= Q1 - 3 * IQR) & (df[i] <= Q3 + 3 *IQR)\n",
    "        df_t = df.loc[filter]\n",
    "        df_t_list.append(i)\n",
    "        df_t_list.append(df_t)\n",
    "    return df_t_list\n",
    "\n",
    "df_t_list = test(df.columns.tolist())\n",
    "for i in df_t_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for many variable, there are relatively much outliers with legal, but extreme values. Therefor the Median aboslute deviation method has been used to get all the outliers.\n",
    "# source code: https://blog.gitnux.com/code/pandas-quantile/\n",
    "\n",
    "def define_data_without_outliers(df, variables_in_data, cut_of_point):\n",
    "    \n",
    "    for i in variables_in_data:\n",
    "        #determine ranges\n",
    "        Q1 = df[i].quantile(0.25)\n",
    "        Q3 = df[i].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # filter outliers from dataset\n",
    "        filter = (df[i] >= Q1 - cut_of_point * IQR) & (df[i] <= Q3 + cut_of_point *IQR)\n",
    "        df_t = df.loc[filter]\n",
    "        print('Filtered data for outliers of variable ' + i + ' with cut-of point ' + str(cut_of_point))    \n",
    "        print(\"hoi\")\n",
    "        print(df_t)\n",
    "    return(df_t)\n",
    "    \n",
    "print(define_data_without_outliers(df, ['RETURN ON ASSETS', 'ESG Combined Score'], 3))\n",
    "#print(df_t.iloc[500:1500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe where probable outliers are deleted\n",
    "# source code: https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-where-column-matches-certain-value \n",
    "# and https://blog.gitnux.com/code/pandas-quantile/\n",
    "def create_index_list_per_var(var_name):\n",
    "    df_t_list = []\n",
    "    indice_list = []\n",
    "    for i in var_name:   \n",
    "        #determine ranges\n",
    "        Q1 = df[i].quantile(0.25)\n",
    "        Q3 = df[i].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        # filter outliers from dataset\n",
    "        filter = (df[i] >= Q1 - 3 * IQR) & (df[i] <= Q3 + 3 *IQR)\n",
    "        indice = df[filter].index.tolist()\n",
    "        indice_list.append(indice)\n",
    "    return df_t_list, indice_list\n",
    "\n",
    "df_t_list, indice_list = create_index_list_per_var(df.columns.tolist())\n",
    "\n",
    "print(indice_list)\n",
    "\n",
    "def create_list_with_outliers(indice_list):\n",
    "    prob_outlier_list = []\n",
    "    for i in range(0,15): \n",
    "        for j in range(0,11069): \n",
    "            if j not in indice_list:\n",
    "                prob_outlier_list.append(j)\n",
    "    return prob_outlier_list\n",
    "\n",
    "outlier_indices = create_list_with_outliers(indice_list)\n",
    "print(len(outlier_indices))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code: https://note.nkmk.me/en/python-pandas-multiple-conditions/ \n",
    "var_under_research = 'Board Size'\n",
    "cut_of_point = 3\n",
    "\n",
    "Q1 = df[var_under_research].quantile(0.25)\n",
    "Q3 = df[var_under_research].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_fence = Q1 - cut_of_point * IQR\n",
    "upper_fence = Q3 + cut_of_point * IQR\n",
    "\n",
    "df_or = df[(df[var_under_research] < lower_fence) | (df[var_under_research] > upper_fence)]\n",
    "\n",
    "print(len(df_or))\n",
    "print(lower_fence)\n",
    "print(upper_fence)\n",
    "# filter outliers from dataset\n",
    "df[var_under_research] >= Q1 - 3 * IQR \n",
    "df[var_under_research] <= Q3 + 3 * IQR\n",
    "\n",
    "\n",
    "df_or.sort_values(by=var_under_research, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with the number of outliers with a cut-of-point of 3\n",
    "\n",
    "# source code: https://note.nkmk.me/en/python-pandas-multiple-conditions/ \n",
    "# and https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-where-column-matches-certain-value \n",
    "# and https://blog.gitnux.com/code/pandas-quantile/\n",
    "import pandas as pd\n",
    "def outliers_per_variable(var_list, cut_of_point): \n",
    "        dataframe_input = []\n",
    "        for i in var_list:\n",
    "            tmp_list = []\n",
    "            Q1 = df[i].quantile(0.25)\n",
    "            Q3 = df[i].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_fence = Q1 - cut_of_point * IQR\n",
    "            upper_fence = Q3 + cut_of_point * IQR\n",
    "            df_or = df[(df[i] < lower_fence) | (df[i] > upper_fence)]\n",
    "            tmp_list.append(i)\n",
    "            tmp_list.append(cut_of_point)\n",
    "            tmp_list.append(lower_fence)\n",
    "            tmp_list.append(upper_fence)\n",
    "            tmp_list.append(len(df_or))\n",
    "            dataframe_input.append(tmp_list)\n",
    "        outlier_statistics = pd.DataFrame(dataframe_input, columns=['variable_name', 'cut_of_point', 'lower_fence', 'upper_fence', 'number_of_outliers'])\n",
    "        return(outlier_statistics)\n",
    "\n",
    "df_outlier = outliers_per_variable(df.columns.tolist(), 3)\n",
    "df_outlier\n",
    "\n",
    "# conclusion: there are some variables with a lot of 'outliers'. However, these outliers are possibly legal outliers. \n",
    "# therefore we delete the rows that are outlier with a maximum of 100 rows per variable, since the boxplots provided the \n",
    "# the insight that for most variables there are only a view outliers between the outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient overview to gain insights on which variables are considered outlier.\n",
    "# source code: https://note.nkmk.me/en/python-pandas-multiple-conditions/ \n",
    "# and https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-where-column-matches-certain-value \n",
    "# and https://blog.gitnux.com/code/pandas-quantile/\n",
    "# and https://stackoverflow.com/questions/29077188/absolute-value-for-column-in-python\n",
    "\n",
    "import pandas as pd\n",
    "def outliers_per_variable(var_list, cut_of_point, number_outliers_deleted): \n",
    "        dataframe_input = []\n",
    "        index_numbers_of_outliers = []\n",
    "        for i in var_list:\n",
    "            tmp_list = []\n",
    "            Q1 = df[i].quantile(0.25)\n",
    "            Q3 = df[i].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_fence = Q1 - cut_of_point * IQR\n",
    "            upper_fence = Q3 + cut_of_point * IQR\n",
    "            df_or = df[(df[i] < lower_fence) | (df[i] > upper_fence)]\n",
    "            df_or = df_or.abs().nlargest(number_outliers_deleted, i)\n",
    "            df_or = df_or.sort_values(by=i, ascending=False)\n",
    "            print(i)\n",
    "            print('de lengte van df_or is ', str(len(df_or)))\n",
    "            print(df_or)\n",
    "            indices = df_or.index.tolist()\n",
    "            print('indices_van de variabele ' + i + 'zijn gelijk aan ' + str(indices) + '. De lengte hiervan is ' + str(len(indices)))\n",
    "        return(df_or)\n",
    "\n",
    "df_outlier = outliers_per_variable(df.columns.tolist(), 3, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient overview to gain insights on which variables are considered outlier.\n",
    "# source code: https://note.nkmk.me/en/python-pandas-multiple-conditions/ \n",
    "# and https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-where-column-matches-certain-value \n",
    "# and https://blog.gitnux.com/code/pandas-quantile/\n",
    "# and https://stackoverflow.com/questions/29077188/absolute-value-for-column-in-python\n",
    "\n",
    "import pandas as pd\n",
    "def outliers_per_variable(var_list, cut_of_point, number_outliers_deleted): \n",
    "        dataframe_input = []\n",
    "        index_numbers_of_outliers = []\n",
    "        for i in var_list:\n",
    "            tmp_list = []\n",
    "            Q1 = df[i].quantile(0.25)\n",
    "            Q3 = df[i].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_fence = Q1 - cut_of_point * IQR\n",
    "            upper_fence = Q3 + cut_of_point * IQR\n",
    "            df_or = df[(df[i] < lower_fence) | (df[i] > upper_fence)]\n",
    "            df_or = df_or.abs().nlargest(number_outliers_deleted, i)\n",
    "            df_or = df_or.sort_values(by=i, ascending=False)\n",
    "            indices = df_or.index.tolist()\n",
    "            for j in indices:\n",
    "                  if j not in index_numbers_of_outliers:\n",
    "                        index_numbers_of_outliers.append(j)\n",
    "        return(index_numbers_of_outliers)\n",
    "\n",
    "index_numbers_of_outliers = outliers_per_variable(df.columns.tolist(), 3, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal number of maximum outliers per variable\n",
    "def research_outlier_boundary_2(start, end, steps, cut_of_point_list):\n",
    "    counter = 0\n",
    "    for i in range(start, end,steps):\n",
    "        for j in cut_of_point_list:\n",
    "            target_number = len(outliers_per_variable(df.columns.tolist(),j,i))\n",
    "            tmp_var = target_number - counter\n",
    "            counter = target_number\n",
    "            print('max number of possible instances with more than one outlier: ' + str((i*15) - target_number) + ' cut-of point: ' + str(j) + ', max number of outliers: ' + str(i) + ', total deleted instances: ' + str(target_number) + ', difference with previous number of deleted instances: ' + str(tmp_var))\n",
    "\n",
    "\n",
    "print(research_outlier_boundary_2(0, 105, 5, [3]))\n",
    "\n",
    "# conclusion: when the max number of outliers increases, eventually the difference of the number of extra deleted instances stays\n",
    "# around 30. The last larger dropt is when the max number of outliers increases from 30 to 35. Therefore, 35 is chosen. 319 outliers\n",
    "# isn't to many so this is considered fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the outliers\n",
    "# rows of outliers are deleted since the max number of possible instances with more than one outlier increases if the max\n",
    "# number of outliers increases. This could mean that other, not as outlier classified values of an instance with an outlier\n",
    "# might involve nearly outlier values. Therefore multiple imputation is less trustworthy and the instances are deleted.\n",
    "\n",
    "df = df.drop(index_numbers_of_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#create list is company names\n",
    "# provides a list of names of all companies included, but gives an error.\n",
    "def create_list_of_companies(df):\n",
    "    tmp_list = []\n",
    "    for i in range(15, len(df)):\n",
    "        company_name = df.iloc[i,0]\n",
    "        if company_name == 'Error in name':\n",
    "            continue\n",
    "        elif company_name not in tmp_list:\n",
    "                tmp_list.append(df.iloc[i,0])\n",
    "    return(tmp_list)\n",
    "\n",
    "company_name_list = create_list_of_companies(df)  \n",
    "\n",
    "def replace_company_names(company_name_list):\n",
    "    counter = 0\n",
    "    for i in range(15, len(df), 15):\n",
    "        index_number = min(counter, (len(df)/15-1))\n",
    "        company_name = company_name_list[index_number]\n",
    "        for j in range(0,15):\n",
    "            row_number = i + j\n",
    "            print(row_number)\n",
    "            print(company_name)\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "print(replace_company_names(company_name_list))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
